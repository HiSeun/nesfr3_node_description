# nesfr3_node_description
'Description of cartographer_node, cartographer_occupancy_grid_node, nesfr3_tracking, and bayes_people_tracker'
This will concretely explain the roles, and the topics that comes in and goes out.

## 1. Cartographer_node
Cartographer_node and Cartographer_occupancy_grid_node are the Google open source libraries. Cartographer is a system that provides real-time SLAM in 2D and 3D across multiple platforms and sensor configurations. Anyone can approach to its sources through [cartographer](https://github.com/cartographer-project/cartographer, "ROS_Wiki").  

### 1.1. Package
Cartographer_node is included in the 'cartographer_ros' package.

### 1.2. Topics
* Subscribes
```
- nesfr3/1/lidar/points
- nesfr3/1/wheel_odom
- nesfr3/1/lidar_imu
```
```nesfr3/1/lidar/points``` is just ```sensor_msgs/PointCloud2``` type message, which contains the lidar points data.   
```nesfr3/1/wheel_odom``` is ```nav_msgs/Odometry``` type message. It would be generated by the encoder located in the wheel we guess.
```nesfr3/1/lidar_imu``` is just ```sensor_msgs/Imu``` type message. We don't know the reason why they changed the name of the topic as lidar_imu.    
It is not clear that whether the nesfr3 robot has the imu sensor itself or not.

* Publishes
```
- nesfr3/1/submap_list
```
```nesfr3/1/submap_list``` is the submap_list topic. Each step, cartograhper will create its own local map. And they save those local maps in this submap_lists and send to the cartographer_occupancy_grid_node. 

### 1.3. Role
The cartographer_node is the SLAM node used for online, real-time SLAM.
Using lidar data, it generates submaps and passes to the occupancy_grid_node.    

* * *
reference : [ROS API reference documentation](https://google-cartographer-ros.readthedocs.io/en/latest/ros_api.html, "google_cartographer")
* * *


## 2. Cartographer_occupancy_grid_node
### 2.1. Package
Cartographer_occupancy_grid_node is included in the 'cartographer_ros' package.

### 2.2. Topics
* Subscribes
```
- nesfr3/1/submap_list
```

* Publishes
```
- nesfr3/1/map
```
```nesfr3/1/map``` is ```nav_msgs/OccupancyGrid``` type. We can visualize the SLAM in RViz by adding the topic name ```nesfr3/1/map```. 

### 2.3. Role
By receiving the submap_lists through the ```nesfr3/1/submap_list``` topic, this node will generate the global map.     
    
<img src = "/Shots/Cartographer1.png" width="300" height="300" align="center"></img>    

As you can see from the above image, the global map is not generated accurately, due to poor loop closure. 

* * *
reference : [ROS API reference documentation](https://google-cartographer-ros.readthedocs.io/en/latest/ros_api.html, "google_cartographer")
* * *


## 3. nesfr3_tracking
nesfr3_tracking node is for **matching process**. There are several function defined in this ```nesfr3_human_matching.py```. Considering these functions, we can define that the two main job for this node are
```
1. Matching cbox and bbox.
2. Matching hsv histogram and the actor id.
```
### 3.1. Package
nesfr3_tracking node is included in the 'nesfr3_tracking' package. 

### 3.2. Topics
* Subscribes
```
- nesfr3/1/fisheye_camera/image_raw/image_topics (/gazebo)
- nesfr3/1/bounding_boxes (image_converter)
- nesfr3/1/object3d_detector/poses (object3d_detector)
- nesfr3/1/cluster (object3d_detector)
- nesfr3/1/people_tracker/people (bayes_people_tracker)
- nesfr3/1/histogram (bayes_people_tracker)
```

* Publishes
```
- nesfr3/1/pose_with_cluster
```
```nesfr3/1/pose_with_cluster``` contains the ```PoseArrayWithClusters.msg``` file. In the message file, we could find the orientation data of the actors ```poses```, and the pointcloud data ```segments```.
    
### 3.3. Roles
The main 4 functions defined in this node are ```onImage(data_bbox, data_cbox, data_cluster)```, ```scoring(cluster_spec, bbox_spec)```, ``` hist_callback(data_img, data_hist, data_state, data_id)``` and ```state_callback(data_img, data_hist, data_state)```.
Starting from ```onImage()``` function we will discuss about the role of this node.

* onImage(data_bbox, data_cbox, data_cluster)
    - Main role of this function is to comparing the cluster box specification and the boundary box specification to identify whether those two boxes are corresponds to the same person. 
    - Through this process, we can filter out the rest of bboxes except for the human. The method for deciding correspondence comes from the following function ```scoring(cluster_spec, bbox_spec)```. 
    
* scoring(cluster_spec, bbox_spec)
    - If the two boxes(cbox, bbox) show the same cluster(human), then the boxes drawn should have the same dimension in x direction. It is x direction because one image came from the fisheye camera are shown in 2D, while the lidar point cloud data has 3D information. 
    - By comparing the x dimension width between each boxes, we could measure the ```x_score``` value through some simple steps. With the high ```x_score``` value was returned, than it shows the high correspondence. 
    
* hist_callback(data_img, data_hist, data_state, data_id)
    - The above two functions were about the boxes, but now it is about the human_id. ```hist_callback()``` function measures the HSV histogram vaule of each image and then make it save in its own repository list(```hist_list```). 
    - Simply, the size of the list was 10. So if the number of the income HSV histogram's value data is over than 10, the first data in the array would be poped out, so the size of the array is maintained with 10. 
    - Like this, everytime 10 hist value will be used in next function, make sure that whether the human was already detected or not.
    
* state_callback(data_img, data_hist, data_state)
    - It compares the current histogram value data with the previous saved 10 data. 
    - It also use 'scoring' method but this scoring is not the same with the above method. Whether the histogram value data was not in the ```hist_list``` data in the above function, than the nesfr3 identifiest the human with ```new_id```.

* * *
reference : [wom-ai/Point Cloud 3d Clustering](https://github.com/wom-ai/nesfr3_workspace/wiki/Point-Cloud-3d-Clustering, "nesfr3_tracking")
* * *

* * *

## shot_controller_node

### package : nesfr3_services

### how it works

shot_controller_node makes nesfr3 enable to track & film certain actor. It requests id of the actor, id of the robot, desired distance & angle between robot and actor, shot size and use_gt(). If use_gt is 1, nesfr3 follows human based on ground truth position of human.

Important functions defined in this node are followed:

#### ShotController(uint32_t robot_id)
 - advertise `("shot_cmd", &shotcontroller::Follow, this)` to make robot follow the actor.

#### Follow(Followactor::request &req, followactor::response &res)

It requests parameter and topic messages required.
In more specific way, this function subscribes actor id and wheel odometry information and then publishes linear/angular velocity and camera tilt/pan.

Parameter requested are:

 `req.actor_id`
 `req.robot_id`
 `req.distance`
 `req.angle`
 `req.shot_size`
 `req.use_gt`

Topic subscribed are:
 `nesfr3/robot_id/Actors`(if `use_gt` =/= 1)
 `gazebo/Actor/1`(if `use_gt` == 1)
 `nesfr3/1/new_id`
 `nesfr3/1/wheel_odom`

And this node publishes:
 `nesfr3/1/gimbal/cmd_pos(geometry_msgs::twist)`
 `nesfr3/1/main_camera/set_hfov(std_msgs::Float64)`
 `nesfr3/1/state(std_msgs::Int32)`
 `nesfr3/1/id(std_msgs::Int32)`

#### NewIdCallback(std_msgs::Int32::ConstPtr &msg)

This function callbacks id of the robot? actor? by `this->actor_id = uint32_t(msg->data)`

#### ActorCallback(nesfr3_msgs::Actor::ConstPtr &msg)

This function gets message of position and orientation of the certain actor(when tracking occurs on the ground truth position of human). And then publishes `cmd_vel` and `cmd_hfov` which refers to robot's angular/linear velocity and camera tilt/pan.
NESFR3 tracks actor with some desired parameters : distance, angle and velocity(to be done), and this function includes how to track actor in efficient way.
If NESFR3 and robot is far away, this function finds a shortest arc route heads for desired position. And then when distance is short enough, robot maintains its velocity and continuously follows target actor.


#### ActorsCallback
This function activates when `use_gt()` isn't equal to 1. It seems to act similar way compared to ActorCallback, but not analyzed yet enough.

#### OdomCallback
This functinon gets robot's current position and orientation information with odometry message received.

#### GetYaw(double x, y, z, w)
This function gets yaw value of the certain actor, by get quaternion value of the orientation and transfer it into roll, pitch and yaw.

* * *

## pcl_transformer_node

 - package : nesfr3_tracking

 - topic subscribes/publishes

subscribes
 - `nesfr3/1/fisheye_camera/image_raw/image_topics`(/gazebo)
 - `nesfr3/1/pose_with_cluster`(nesfr3_tracking)

publishes
 - `nesfr3/1/blobsarray`(/nesfr3/1/bayes_people_tracker)

 - how it works
 
This node gets image from fisheye_camera and pose_with_cluster message from nesfr3_tracking. This message contains orientation data of each actors detected, and point cloud cluster information, and then this node projects point cloud information into the camera image, by coordinate transition from 3-dimensional cartesian coordinate to polar coordinate and 2-dimensional cartesian coordinate on the image.

Important functions defined are followed:

#### dataCallback(const nesfr3_msgs::PoseArrayWithClusters::ConstPtr &clusters_msg, const sensor_msgs::ImageConstPtr &img_msg, ros::Publisher blobs_array_pub_, bool verbose)

This function callbacks required messages which are `clusters_msg`(point cloud clsuter contains actors' orientation) and `img_msg`(fisheye camera image) and publishes blobsArray message. This message contains poses, tracking_id of each cluster converted into image blob.

#### cart2img

'Image blob' described on the above line is shown on this function. This function calls `xyz2rt2image()` function to convert point cloud cluster into image blob, means rectangular zone has identical center with point cloud. Then it publishes `blobsArray` which is array that contains each blob's center coordinate, width and height.

#### xyz2rt2image

This function calculates coordinate of center point of transformed image blob. Mathematical calculation is included, further analysis is needed for better understanding.

## 4. bayes_people_tracker
### 4.1. Package
```bayes_people_tracker``` is contained in the ```bayes_people_tracker``` package. And this package uses the bayes_tracking library developed by Nicola Bellotto (University of Lincoln).

### 4.2. Topics
* Subscribes
```
- nesfr3/1/blobsArray (pcl_transformer_node)
```
From ```pcl_transformer_node```, it receives the ```BlobsArray.msg```, and using ```connectCallback()``` and ```detectCallback()``` function it deals with the tracking ids. (I guess)

* Publishes
```
- nesfr3/1/people_tracker/people
- nesfr3/1/histogram
```
```bayes_people_tracker``` gathers the each people's position and velocity data and passes to the ```nesfr3_tracking``` as topic name ```nesfr3/1/people_tracker/people```.    
Also, it publishes another topic name ```nesfr3/1/histogram``` which contains the data of tracking id(blobsArray indexes).    

### 4.3. Role
This node receives inputs from the user, which ```filter``` they will use and what would be the ```stdlimit``` value, and the constant velocity model noise ```cv_model_noise```. Also you can decide the matching algorithm would be either **NN**(Nearest Neighborhood) or **NNJPDA**(Nearest Neighbor Joint Probabilistic Data Association).    
Depending on the input filter which we would use, this node tracks the pose, and geograhpical position of the people, and then estimates them.   
EKF, UKF, and PF filter can be used.    

- This node is runned by executing ```object3d_detector.launch```. And the **all inputs need for running the ```bayes_people_tracker``` was defined in ```nesfr3/nesfr3_tracking/object3d_detector/config/object3d_detector.yaml``` file**. Therefore if you want to change filter type or the model noise value, simply you can **modify the number or the filter type in the yaml file.**   
   
- Finally you can customize your own filter to track & estimate the human actor position / pose. 
